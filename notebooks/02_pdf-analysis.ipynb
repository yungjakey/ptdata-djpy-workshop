{
    "cells": [
        {
            "cell_type": "markdown",
            "id": "2730eb5e",
            "metadata": {},
            "source": [
                "# Text Analysis Notebook\n",
                "\n",
                "This notebook performs a comprehensive text analysis on a government program PDF. The analysis includes PDF processing, token statistics, part-of-speech and named entity analyses, word cloud generation, keyword extraction, entity network visualization, document structure analysis, and more."
            ]
        },
        {
            "cell_type": "markdown",
            "id": "34b49531",
            "metadata": {},
            "source": [
                "## 0. Setup "
            ]
        },
        {
            "cell_type": "markdown",
            "id": "a2165af5",
            "metadata": {},
            "source": [
                "### Get data\n",
                "\n",
                "\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 1,
            "id": "7a41e4e0",
            "metadata": {},
            "outputs": [],
            "source": [
                "# import requests\n",
                "# url = 'https://b.ds.at/lido-files/2025/02/27/3dfd41d6-f95a-44f3-90b0-c6b15eff07de.pdf'\n",
                "\n",
                "# r = requests.get(url, stream=True) # stream=True allows downloading large files in pieces\n",
                "# chunk_size = 2000 # size of each chunk in bytes\n",
                "\n",
                "# with open('../../data02_pdf-analysis/Regierungsprogramm_2025.pdf', 'wb+') as fd: # wb = write binary\n",
                "#     for chunk in r.iter_content(chunk_size):\n",
                "#         fd.write(chunk)\n",
                "\n",
                "\n",
                "# alternatively, if file is small enough, you can use the following code to download the file\n",
                "# with open('regierungsprogram.pdf', 'wb') as f:\n",
                "#     f.write(r.content)"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "c0089b96",
            "metadata": {},
            "source": [
                "### Install pdfplumber"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 2,
            "id": "d2e1dca0",
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Collecting pdfplumber\n",
                        "  Using cached pdfplumber-0.11.5-py3-none-any.whl.metadata (42 kB)\n",
                        "Collecting pdfminer.six==20231228 (from pdfplumber)\n",
                        "  Using cached pdfminer.six-20231228-py3-none-any.whl.metadata (4.2 kB)\n",
                        "Collecting Pillow>=9.1 (from pdfplumber)\n",
                        "  Using cached pillow-11.1.0-cp311-cp311-macosx_11_0_arm64.whl.metadata (9.1 kB)\n",
                        "Collecting pypdfium2>=4.18.0 (from pdfplumber)\n",
                        "  Using cached pypdfium2-4.30.1-py3-none-macosx_11_0_arm64.whl.metadata (48 kB)\n",
                        "Requirement already satisfied: charset-normalizer>=2.0.0 in /Users/jk/Library/Caches/pypoetry/virtualenvs/ptdata-social-media-enrichment-zyRWzRD1-py3.11/lib/python3.11/site-packages (from pdfminer.six==20231228->pdfplumber) (3.4.0)\n",
                        "Requirement already satisfied: cryptography>=36.0.0 in /Users/jk/Library/Caches/pypoetry/virtualenvs/ptdata-social-media-enrichment-zyRWzRD1-py3.11/lib/python3.11/site-packages (from pdfminer.six==20231228->pdfplumber) (43.0.3)\n",
                        "Requirement already satisfied: cffi>=1.12 in /Users/jk/Library/Caches/pypoetry/virtualenvs/ptdata-social-media-enrichment-zyRWzRD1-py3.11/lib/python3.11/site-packages (from cryptography>=36.0.0->pdfminer.six==20231228->pdfplumber) (1.17.1)\n",
                        "Requirement already satisfied: pycparser in /Users/jk/Library/Caches/pypoetry/virtualenvs/ptdata-social-media-enrichment-zyRWzRD1-py3.11/lib/python3.11/site-packages (from cffi>=1.12->cryptography>=36.0.0->pdfminer.six==20231228->pdfplumber) (2.22)\n",
                        "Using cached pdfplumber-0.11.5-py3-none-any.whl (59 kB)\n",
                        "Using cached pdfminer.six-20231228-py3-none-any.whl (5.6 MB)\n",
                        "Using cached pillow-11.1.0-cp311-cp311-macosx_11_0_arm64.whl (3.1 MB)\n",
                        "Using cached pypdfium2-4.30.1-py3-none-macosx_11_0_arm64.whl (2.8 MB)\n",
                        "Installing collected packages: pypdfium2, Pillow, pdfminer.six, pdfplumber\n",
                        "Successfully installed Pillow-11.1.0 pdfminer.six-20231228 pdfplumber-0.11.5 pypdfium2-4.30.1\n",
                        "\n",
                        "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.3.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.0.1\u001b[0m\n",
                        "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49m/Users/jk/Library/Caches/pypoetry/virtualenvs/ptdata-social-media-enrichment-zyRWzRD1-py3.11/bin/python -m pip install --upgrade pip\u001b[0m\n",
                        "Note: you may need to restart the kernel to use updated packages.\n"
                    ]
                }
            ],
            "source": [
                "%pip install pdfplumber"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "a5541172",
            "metadata": {},
            "source": [
                "### Install language corpus"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 3,
            "id": "dc450a76",
            "metadata": {},
            "outputs": [
                {
                    "ename": "NameError",
                    "evalue": "name 'spacy' is not defined",
                    "output_type": "error",
                    "traceback": [
                        "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
                        "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
                        "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[3]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# Check if model is already installed\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33mde_core_news_lg\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m \u001b[43mspacy\u001b[49m.util.get_installed_models():\n\u001b[32m      3\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mInstalling de_core_news_lg model...\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m      4\u001b[39m     get_ipython().system(\u001b[33m'\u001b[39m\u001b[33mpython -m spacy download de_core_news_lg\u001b[39m\u001b[33m'\u001b[39m)\n",
                        "\u001b[31mNameError\u001b[39m: name 'spacy' is not defined"
                    ]
                }
            ],
            "source": [
                "# Check if model is already installed\n",
                "if \"de_core_news_lg\" not in spacy.util.get_installed_models():\n",
                "    print(\"Installing de_core_news_lg model...\")\n",
                "    !python -m spacy download de_core_news_lg\n",
                "else:\n",
                "    print(\"de_core_news_lg already installed!\")"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "f10cb76e",
            "metadata": {},
            "source": [
                "### Imports"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "baca332b",
            "metadata": {},
            "outputs": [],
            "source": [
                "# Import necessary libraries\n",
                "import re\n",
                "import spacy\n",
                "import pandas as pd\n",
                "import numpy as np\n",
                "import matplotlib.pyplot as plt\n",
                "import seaborn as sns\n",
                "import pdfplumber as pp\n",
                "from collections import Counter\n",
                "\n",
                "from wordcloud import WordCloud\n",
                "\n",
                "# Set visualization style\n",
                "plt.style.use(\"ggplot\")\n",
                "plt.rcParams[\"figure.figsize\"] = (14, 8)"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "78ed6c0f",
            "metadata": {},
            "source": [
                "## 1.2 Read data"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "70838d16",
            "metadata": {},
            "outputs": [],
            "source": [
                "# Load the German language model\n",
                "print(\"Loading German language model...\")\n",
                "german_lang = spacy.load(\"de_core_news_lg\")\n",
                "german_lang.add_pipe(\"sentencizer\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "b1970db6",
            "metadata": {},
            "outputs": [],
            "source": [
                "pdf_path = \"../data02_pdf-analysis/Regierungsprogramm_2025.pdf\"\n",
                "\n",
                "with pp.open(pdf_path) as pdf:\n",
                "    text = \"\\n\".join([page.extract_text() for page in pdf.pages if page.extract_text()])\n",
                "\n",
                "text = re.sub(r\"\\n+\", \" \", re.sub(r\"\\s+\", \" \", text))\n",
                "doc = german_lang(text)"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "52d5b9ec",
            "metadata": {},
            "source": [
                "## 1.2 Word Cloud"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "9404cadf",
            "metadata": {},
            "outputs": [],
            "source": [
                "print(\"\\nGenerating word cloud...\")\n",
                "\n",
                "\n",
                "def generate_wordcloud(text, title=\"Word Cloud\", stopwords=None):\n",
                "    # Filter out stopwords and punctuation\n",
                "    if stopwords is None:\n",
                "        stopwords = []\n",
                "\n",
                "    # Create and generate a word cloud image\n",
                "    wordcloud = WordCloud(\n",
                "        width=800,\n",
                "        height=400,\n",
                "        background_color=\"white\",\n",
                "        stopwords=stopwords,\n",
                "        min_font_size=10,\n",
                "        max_font_size=150,\n",
                "        collocations=False,\n",
                "        random_state=42,\n",
                "    ).generate(text)\n",
                "\n",
                "    # Display the generated image\n",
                "    plt.figure(figsize=(10, 5))\n",
                "    plt.imshow(wordcloud, interpolation=\"bilinear\")\n",
                "    plt.axis(\"off\")\n",
                "    plt.title(title, fontsize=16)\n",
                "    plt.tight_layout()\n",
                "    plt.show()\n",
                "\n",
                "\n",
                "# Create a list of tokens excluding stopwords and punctuation\n",
                "filtered_tokens = [\n",
                "    token.text.lower()\n",
                "    for token in doc\n",
                "    if not token.is_stop and not token.is_punct and token.is_alpha\n",
                "]\n",
                "filtered_text = \" \".join(filtered_tokens)\n",
                "\n",
                "# Generate word cloud\n",
                "generate_wordcloud(filtered_text, title=\"Most Frequent Words in the Government Program\")"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "97efc102",
            "metadata": {},
            "source": [
                "## 1.3 Most Common Words"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "2d7167dc",
            "metadata": {},
            "outputs": [],
            "source": [
                "print(\"\\nAnalyzing most common nouns...\")\n",
                "nouns = [\n",
                "    token.text.lower() for token in doc if token.pos_ == \"NOUN\" and not token.is_stop\n",
                "]\n",
                "noun_counts = Counter(nouns).most_common(20)\n",
                "noun_df = pd.DataFrame(noun_counts, columns=[\"Noun\", \"Count\"])\n",
                "\n",
                "plt.figure(figsize=(12, 6))\n",
                "sns.barplot(x=\"Count\", y=\"Noun\", data=noun_df)\n",
                "plt.title(\"Most Frequent Nouns\", fontsize=16)\n",
                "plt.xlabel(\"Count\", fontsize=14)\n",
                "plt.ylabel(\"Nouns\", fontsize=14)\n",
                "plt.tight_layout()\n",
                "plt.show()"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "d0f5b141",
            "metadata": {},
            "outputs": [],
            "source": [
                "print(\"\\nAnalyzing most common adjectives...\")\n",
                "adjectives = [\n",
                "    token.text.lower() for token in doc if token.pos_ == \"ADJ\" and not token.is_stop\n",
                "]\n",
                "adj_counts = Counter(adjectives).most_common(20)\n",
                "adj_df = pd.DataFrame(adj_counts, columns=[\"Adjective\", \"Count\"])\n",
                "\n",
                "plt.figure(figsize=(12, 6))\n",
                "sns.barplot(x=\"Count\", y=\"Adjective\", data=adj_df)\n",
                "plt.title(\"Most Frequent Adjectives\", fontsize=16)\n",
                "plt.xlabel(\"Count\", fontsize=14)\n",
                "plt.ylabel(\"Adjectives\", fontsize=14)\n",
                "plt.tight_layout()\n",
                "plt.show()"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "f9d09f91",
            "metadata": {},
            "source": [
                "### 1.4 Sentence Length Analysis"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "ee1b4ad1",
            "metadata": {},
            "outputs": [],
            "source": [
                "sentence_lengths = [len(sent) for sent in doc.sents]\n",
                "\n",
                "plt.figure(figsize=(14, 6))\n",
                "plt.hist(sentence_lengths, bins=50, alpha=0.7, color=\"steelblue\")\n",
                "plt.axvline(\n",
                "    x=np.mean(sentence_lengths),\n",
                "    color=\"red\",\n",
                "    linestyle=\"--\",\n",
                "    label=f\"Average: {np.mean(sentence_lengths):.1f} Tokens\",\n",
                ")\n",
                "plt.title(\"Distribution of Sentence Lengths\", fontsize=16)\n",
                "plt.xlabel(\"Number of Tokens per Sentence\", fontsize=14)\n",
                "plt.ylabel(\"Frequency\", fontsize=14)\n",
                "plt.legend()\n",
                "plt.tight_layout()\n",
                "plt.show()"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "707e706c",
            "metadata": {},
            "source": [
                "### 1.5 Summary"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "a212224f",
            "metadata": {},
            "outputs": [],
            "source": [
                "print(\"\\n=== Summary of Text Analysis ===\")\n",
                "print(f\"Document Length: {len(doc)} Tokens\")\n",
                "print(\n",
                "    f\"Unique Words: {len(set([token.text.lower() for token in doc if token.is_alpha]))}\"\n",
                ")\n",
                "print(f\"Number of Sentences: {len(list(doc.sents))}\")\n",
                "print(f\"Average Sentence Length: {np.mean(sentence_lengths):.1f} Tokens\")\n",
                "print(f\"Named Entities Found: {len(doc.ents)}\")"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "ptdata-social-media-enrichment",
            "language": "python",
            "name": "ptdata-social-media-enrichment"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.11.10"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 5
}
