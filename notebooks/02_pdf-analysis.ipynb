{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2730eb5e",
   "metadata": {},
   "source": [
    "# Text Analysis Notebook\n",
    "\n",
    "This notebook performs a comprehensive text analysis on a government program PDF. The analysis includes PDF processing, token statistics, part-of-speech and named entity analyses, word cloud generation, keyword extraction, entity network visualization, document structure analysis, and more."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "baca332b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import re\n",
    "import spacy\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pdfplumber as pp\n",
    "from collections import Counter\n",
    "\n",
    "from wordcloud import WordCloud\n",
    "\n",
    "# Set visualization style\n",
    "plt.style.use(\"ggplot\")\n",
    "plt.rcParams[\"figure.figsize\"] = (14, 8)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34b49531",
   "metadata": {},
   "source": [
    "## 0. Setup "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2165af5",
   "metadata": {},
   "source": [
    "### Get data\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a41e4e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import requests\n",
    "# url = 'https://b.ds.at/lido-files/2025/02/27/3dfd41d6-f95a-44f3-90b0-c6b15eff07de.pdf'\n",
    "\n",
    "# r = requests.get(url, stream=True) # stream=True allows downloading large files in pieces\n",
    "# chunk_size = 2000 # size of each chunk in bytes\n",
    "\n",
    "# with open('../data/02_pdf-analysis/Regierungsprogramm_2025.pdf', 'wb+') as fd: # wb = write binary\n",
    "#     for chunk in r.iter_content(chunk_size):\n",
    "#         fd.write(chunk)\n",
    "\n",
    "\n",
    "# alternatively, if file is small enough, you can use the following code to download the file\n",
    "# with open('regierungsprogram.pdf', 'wb') as f:\n",
    "#     f.write(r.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0089b96",
   "metadata": {},
   "source": [
    "## Install pdfplumber"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2e1dca0",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install pdfplumber"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5541172",
   "metadata": {},
   "source": [
    "### Install language corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc450a76",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if model is already installed\n",
    "if \"de_core_news_lg\" not in spacy.util.get_installed_models():\n",
    "    print(\"Installing de_core_news_lg model...\")\n",
    "    !python -m spacy download de_core_news_lg\n",
    "else:\n",
    "    print(\"de_core_news_lg already installed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78ed6c0f",
   "metadata": {},
   "source": [
    "## 1.2 Read data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70838d16",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the German language model\n",
    "print(\"Loading German language model...\")\n",
    "german_lang = spacy.load(\"de_core_news_lg\")\n",
    "german_lang.add_pipe(\"sentencizer\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1970db6",
   "metadata": {},
   "outputs": [],
   "source": [
    "pdf_path = \"data/02_pdf-analysis/Regierungsprogramm_2025.pdf\"\n",
    "\n",
    "with pp.open(pdf_path) as pdf:\n",
    "    text = \"\\n\".join([page.extract_text() for page in pdf.pages if page.extract_text()])\n",
    "\n",
    "text = re.sub(r\"\\n+\", \" \", re.sub(r\"\\s+\", \" \", text))\n",
    "doc = german_lang(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52d5b9ec",
   "metadata": {},
   "source": [
    "## 1.2 Word Cloud"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9404cadf",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nGenerating word cloud...\")\n",
    "\n",
    "\n",
    "def generate_wordcloud(text, title=\"Word Cloud\", stopwords=None):\n",
    "    # Filter out stopwords and punctuation\n",
    "    if stopwords is None:\n",
    "        stopwords = []\n",
    "\n",
    "    # Create and generate a word cloud image\n",
    "    wordcloud = WordCloud(\n",
    "        width=800,\n",
    "        height=400,\n",
    "        background_color=\"white\",\n",
    "        stopwords=stopwords,\n",
    "        min_font_size=10,\n",
    "        max_font_size=150,\n",
    "        collocations=False,\n",
    "        random_state=42,\n",
    "    ).generate(text)\n",
    "\n",
    "    # Display the generated image\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    plt.imshow(wordcloud, interpolation=\"bilinear\")\n",
    "    plt.axis(\"off\")\n",
    "    plt.title(title, fontsize=16)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "# Create a list of tokens excluding stopwords and punctuation\n",
    "filtered_tokens = [\n",
    "    token.text.lower()\n",
    "    for token in doc\n",
    "    if not token.is_stop and not token.is_punct and token.is_alpha\n",
    "]\n",
    "filtered_text = \" \".join(filtered_tokens)\n",
    "\n",
    "# Generate word cloud\n",
    "generate_wordcloud(filtered_text, title=\"Most Frequent Words in the Government Program\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97efc102",
   "metadata": {},
   "source": [
    "## 1.3 Most Common Words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d7167dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nAnalyzing most common nouns...\")\n",
    "nouns = [\n",
    "    token.text.lower() for token in doc if token.pos_ == \"NOUN\" and not token.is_stop\n",
    "]\n",
    "noun_counts = Counter(nouns).most_common(20)\n",
    "noun_df = pd.DataFrame(noun_counts, columns=[\"Noun\", \"Count\"])\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "sns.barplot(x=\"Count\", y=\"Noun\", data=noun_df)\n",
    "plt.title(\"Most Frequent Nouns\", fontsize=16)\n",
    "plt.xlabel(\"Count\", fontsize=14)\n",
    "plt.ylabel(\"Nouns\", fontsize=14)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0f5b141",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nAnalyzing most common adjectives...\")\n",
    "adjectives = [\n",
    "    token.text.lower() for token in doc if token.pos_ == \"ADJ\" and not token.is_stop\n",
    "]\n",
    "adj_counts = Counter(adjectives).most_common(20)\n",
    "adj_df = pd.DataFrame(adj_counts, columns=[\"Adjective\", \"Count\"])\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "sns.barplot(x=\"Count\", y=\"Adjective\", data=adj_df)\n",
    "plt.title(\"Most Frequent Adjectives\", fontsize=16)\n",
    "plt.xlabel(\"Count\", fontsize=14)\n",
    "plt.ylabel(\"Adjectives\", fontsize=14)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9d09f91",
   "metadata": {},
   "source": [
    "### 1.4 Sentence Length Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee1b4ad1",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence_lengths = [len(sent) for sent in doc.sents]\n",
    "\n",
    "plt.figure(figsize=(14, 6))\n",
    "plt.hist(sentence_lengths, bins=50, alpha=0.7, color=\"steelblue\")\n",
    "plt.axvline(\n",
    "    x=np.mean(sentence_lengths),\n",
    "    color=\"red\",\n",
    "    linestyle=\"--\",\n",
    "    label=f\"Average: {np.mean(sentence_lengths):.1f} Tokens\",\n",
    ")\n",
    "plt.title(\"Distribution of Sentence Lengths\", fontsize=16)\n",
    "plt.xlabel(\"Number of Tokens per Sentence\", fontsize=14)\n",
    "plt.ylabel(\"Frequency\", fontsize=14)\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "707e706c",
   "metadata": {},
   "source": [
    "### 1.5 Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a212224f",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n=== Summary of Text Analysis ===\")\n",
    "print(f\"Document Length: {len(doc)} Tokens\")\n",
    "print(\n",
    "    f\"Unique Words: {len(set([token.text.lower() for token in doc if token.is_alpha]))}\"\n",
    ")\n",
    "print(f\"Number of Sentences: {len(list(doc.sents))}\")\n",
    "print(f\"Average Sentence Length: {np.mean(sentence_lengths):.1f} Tokens\")\n",
    "print(f\"Named Entities Found: {len(doc.ents)}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "djpyworkshop",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
