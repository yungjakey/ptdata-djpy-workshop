{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# Parliamentary Text Analysis\n",
                "\n",
                "This notebook analyzes parliamentary text data using advanced NLP techniques including topic modeling, entity recognition, and network analysis."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Import required libraries\n",
                "import pandas as pd\n",
                "import numpy as np\n",
                "import matplotlib.pyplot as plt\n",
                "import seaborn as sns\n",
                "from collections import Counter\n",
                "import re\n",
                "import spacy\n",
                "from wordcloud import WordCloud\n",
                "import string\n",
                "import networkx as nx\n",
                "from sklearn.feature_extraction.text import TfidfVectorizer\n",
                "from sklearn.decomposition import LatentDirichletAllocation\n",
                "import pyLDAvis\n",
                "import pyLDAvis.sklearn\n",
                "\n",
                "# Set visualization styles\n",
                "plt.style.use('ggplot')\n",
                "sns.set_palette(\"viridis\")\n",
                "sns.set_context(\"notebook\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 0. Setup "
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### Get data\n",
                "\n",
                "\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "%%bash\n",
                "#"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### Install language corpus"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Collecting de-core-news-lg==3.8.0\n",
                        "  Downloading https://github.com/explosion/spacy-models/releases/download/de_core_news_lg-3.8.0/de_core_news_lg-3.8.0-py3-none-any.whl (567.8 MB)\n",
                        "\u001b[2K     \u001b[91m━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━�━\u001b[0m \u001b[32m0.0/567.8 MB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m��━━━━━━━━━━━\u001b[0m \u001b[32m0.3/567.8 MB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m��━━━━━━━━━━━\u001b[0m \u001b[32m0.5/567.8 MB\u001b[0m \u001b[31m1.5 MB/s\u001b[0m eta \u001b[36m0:06:21\u001b[0m��━━━━━━━━━━━\u001b[0m \u001b[32m0.8/567.8 MB\u001b[0m \u001b[31m1.3 MB/s\u001b[0m eta \u001b[36m0:07:02\u001b[0m��━━━━━━━━━━━\u001b[0m \u001b[32m1.0/567.8 MB\u001b[0m \u001b[31m1.4 MB/s\u001b[0m eta \u001b[36m0:06:39\u001b[0m��━━━━━━━━━━━\u001b[0m \u001b[32m1.6/567.8 MB\u001b[0m \u001b[31m1.7 MB/s\u001b[0m eta \u001b[36m0:05:37\u001b[0m��━━━━━━━━━━━\u001b[0m \u001b[32m1.8/567.8 MB\u001b[0m \u001b[31m1.5 MB/s\u001b[0m eta \u001b[36m0:06:11\u001b[0m��━━━━━━━━━━━\u001b[0m \u001b[32m2.1/567.8 MB\u001b[0m \u001b[31m1.5 MB/s\u001b[0m eta \u001b[36m0:06:19\u001b[0m��━━━━━━━━━━━\u001b[0m \u001b[32m2.1/567.8 MB\u001b[0m \u001b[31m1.5 MB/s\u001b[0m eta \u001b[36m0:06:19\u001b[0m��━━━━━━━━━━━\u001b[0m \u001b[32m2.6/567.8 MB\u001b[0m \u001b[31m1.5 MB/s\u001b[0m eta \u001b[36m0:06:24\u001b[0m��━━━━━━━━━━━\u001b[0m \u001b[32m3.4/567.8 MB\u001b[0m \u001b[31m1.7 MB/s\u001b[0m eta \u001b[36m0:05:26\u001b[0m��━━━━━━━━━━━\u001b[0m \u001b[32m3.7/567.8 MB\u001b[0m \u001b[31m1.7 MB/s\u001b[0m eta \u001b[36m0:05:24\u001b[0m��━━━━━━━━━━━\u001b[0m \u001b[32m4.2/567.8 MB\u001b[0m \u001b[31m1.8 MB/s\u001b[0m eta \u001b[36m0:05:17\u001b[0m��━━━━━━━━━━━\u001b[0m \u001b[32m4.5/567.8 MB\u001b[0m \u001b[31m1.8 MB/s\u001b[0m eta \u001b[36m0:05:11\u001b[0m��━━━━━━━━━━━\u001b[0m \u001b[32m5.0/567.8 MB\u001b[0m \u001b[31m1.8 MB/s\u001b[0m eta \u001b[36m0:05:11\u001b[0m��━━━━━━━━━━━\u001b[0m \u001b[32m5.5/567.8 MB\u001b[0m \u001b[31m1.9 MB/s\u001b[0m eta \u001b[36m0:04:59\u001b[0m��━━━━━━━━━━━\u001b[0m \u001b[32m6.3/567.8 MB\u001b[0m \u001b[31m2.0 MB/s\u001b[0m eta \u001b[36m0:04:43\u001b[0m��━━━━━━━━━━━\u001b[0m \u001b[32m6.6/567.8 MB\u001b[0m \u001b[31m1.9 MB/s\u001b[0m eta \u001b[36m0:04:48\u001b[0m��━━━━━━━━━━━\u001b[0m \u001b[32m7.1/567.8 MB\u001b[0m \u001b[31m2.0 MB/s\u001b[0m eta \u001b[36m0:04:41\u001b[0m��━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.3/567.8 MB\u001b[0m \u001b[31m2.0 MB/s\u001b[0m eta \u001b[36m0:04:40\u001b[0m��━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.9/567.8 MB\u001b[0m \u001b[31m2.0 MB/s\u001b[0m eta \u001b[36m0:04:45\u001b[0m��━━━━━━━━━━━━━━\u001b[0m \u001b[32m8.7/567.8 MB\u001b[0m \u001b[31m2.1 MB/s\u001b[0m eta \u001b[36m0:04:29\u001b[0m��━━━━━━━━━━━━━━\u001b[0m \u001b[32m9.2/567.8 MB\u001b[0m \u001b[31m2.1 MB/s\u001b[0m eta \u001b[36m0:04:24\u001b[0m��━━━━━━━━━━━━━━\u001b[0m \u001b[32m9.7/567.8 MB\u001b[0m \u001b[31m2.1 MB/s\u001b[0m eta \u001b[36m0:04:23\u001b[0m��━━━━━━━━━━━━━━\u001b[0m \u001b[32m10.5/567.8 MB\u001b[0m \u001b[31m2.2 MB/s\u001b[0m eta \u001b[36m0:04:18\u001b[0m��━━━━━━━━━━━━━━\u001b[0m \u001b[32m10.7/567.8 MB\u001b[0m \u001b[31m2.2 MB/s\u001b[0m eta \u001b[36m0:04:16\u001b[0m��━━━━━━━━━━━━━━\u001b[0m \u001b[32m11.3/567.8 MB\u001b[0m \u001b[31m2.2 MB/s\u001b[0m eta \u001b[36m0:04:19\u001b[0m��━━━━━━━━━━━━━━\u001b[0m \u001b[32m11.5/567.8 MB\u001b[0m \u001b[31m2.1 MB/s\u001b[0m eta \u001b[36m0:04:20\u001b[0m��━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.1/567.8 MB\u001b[0m \u001b[31m2.1 MB/s\u001b[0m eta \u001b[36m0:04:19\u001b[0m��━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.6/567.8 MB\u001b[0m \u001b[31m2.1 MB/s\u001b[0m eta \u001b[36m0:04:19\u001b[0m��━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.8/567.8 MB\u001b[0m \u001b[31m2.1 MB/s\u001b[0m eta \u001b[36m0:04:20\u001b[0m��━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.8/567.8 MB\u001b[0m \u001b[31m2.1 MB/s\u001b[0m eta \u001b[36m0:04:20\u001b[0m��━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.8/567.8 MB\u001b[0m \u001b[31m2.1 MB/s\u001b[0m eta \u001b[36m0:04:20\u001b[0m��━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.1/567.8 MB\u001b[0m \u001b[31m2.0 MB/s\u001b[0m eta \u001b[36m0:04:39\u001b[0m��━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.4/567.8 MB\u001b[0m \u001b[31m2.0 MB/s\u001b[0m eta \u001b[36m0:04:43\u001b[0m��━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.4/567.8 MB\u001b[0m \u001b[31m2.0 MB/s\u001b[0m eta \u001b[36m0:04:43\u001b[0m��━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.6/567.8 MB\u001b[0m \u001b[31m1.9 MB/s\u001b[0m eta \u001b[36m0:04:50\u001b[0m��━━━━━━━━━━━━━━\u001b[0m \u001b[32m14.2/567.8 MB\u001b[0m \u001b[31m1.9 MB/s\u001b[0m eta \u001b[36m0:04:53\u001b[0m��━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m14.7/567.8 MB\u001b[0m \u001b[31m1.9 MB/s\u001b[0m eta \u001b[36m0:04:47\u001b[0m��━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m15.2/567.8 MB\u001b[0m \u001b[31m1.9 MB/s\u001b[0m eta \u001b[36m0:04:45\u001b[0m��━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m16.0/567.8 MB\u001b[0m \u001b[31m2.0 MB/s\u001b[0m eta \u001b[36m0:04:38\u001b[0m��━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m17.0/567.8 MB\u001b[0m \u001b[31m2.1 MB/s\u001b[0m eta \u001b[36m0:04:29\u001b[0m��━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m17.8/567.8 MB\u001b[0m \u001b[31m2.1 MB/s\u001b[0m eta \u001b[36m0:04:24\u001b[0m��━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m18.4/567.8 MB\u001b[0m \u001b[31m2.1 MB/s\u001b[0m eta \u001b[36m0:04:21\u001b[0m��━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m18.6/567.8 MB\u001b[0m \u001b[31m2.1 MB/s\u001b[0m eta \u001b[36m0:04:20\u001b[0m��━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m18.9/567.8 MB\u001b[0m \u001b[31m2.1 MB/s\u001b[0m eta \u001b[36m0:04:24\u001b[0m��━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m19.9/567.8 MB\u001b[0m \u001b[31m2.1 MB/s\u001b[0m eta \u001b[36m0:04:17\u001b[0m��━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m20.4/567.8 MB\u001b[0m \u001b[31m2.2 MB/s\u001b[0m eta \u001b[36m0:04:14\u001b[0m��━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.5/567.8 MB\u001b[0m \u001b[31m2.2 MB/s\u001b[0m eta \u001b[36m0:04:08\u001b[0m��━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.8/567.8 MB\u001b[0m \u001b[31m2.2 MB/s\u001b[0m eta \u001b[36m0:04:08\u001b[0m��━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m22.5/567.8 MB\u001b[0m \u001b[31m2.2 MB/s\u001b[0m eta \u001b[36m0:04:06\u001b[0m��━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m22.8/567.8 MB\u001b[0m \u001b[31m2.2 MB/s\u001b[0m eta \u001b[36m0:04:05\u001b[0m��━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m23.1/567.8 MB\u001b[0m \u001b[31m2.2 MB/s\u001b[0m eta \u001b[36m0:04:07\u001b[0m��━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m23.3/567.8 MB\u001b[0m \u001b[31m2.2 MB/s\u001b[0m eta \u001b[36m0:04:11\u001b[0m��━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m23.9/567.8 MB\u001b[0m \u001b[31m2.2 MB/s\u001b[0m eta \u001b[36m0:04:09\u001b[0m��━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.4/567.8 MB\u001b[0m \u001b[31m2.2 MB/s\u001b[0m eta \u001b[36m0:04:07\u001b[0m��━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.9/567.8 MB\u001b[0m \u001b[31m2.2 MB/s\u001b[0m eta \u001b[36m0:04:07\u001b[0m��━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m25.7/567.8 MB\u001b[0m \u001b[31m2.2 MB/s\u001b[0m eta \u001b[36m0:04:04\u001b[0m��━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m26.5/567.8 MB\u001b[0m \u001b[31m2.3 MB/s\u001b[0m eta \u001b[36m0:04:01\u001b[0m��━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m27.0/567.8 MB\u001b[0m \u001b[31m2.3 MB/s\u001b[0m eta \u001b[36m0:04:00\u001b[0m��━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m27.3/567.8 MB\u001b[0m \u001b[31m2.2 MB/s\u001b[0m eta \u001b[36m0:04:01\u001b[0m��━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m27.5/567.8 MB\u001b[0m \u001b[31m2.2 MB/s\u001b[0m eta \u001b[36m0:04:01\u001b[0m��━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m28.0/567.8 MB\u001b[0m \u001b[31m2.2 MB/s\u001b[0m eta \u001b[36m0:04:02\u001b[0m��━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m28.8/567.8 MB\u001b[0m \u001b[31m2.3 MB/s\u001b[0m eta \u001b[36m0:04:00\u001b[0m��━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m29.1/567.8 MB\u001b[0m \u001b[31m2.2 MB/s\u001b[0m eta \u001b[36m0:04:01\u001b[0m��━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m29.6/567.8 MB\u001b[0m \u001b[31m2.2 MB/s\u001b[0m eta \u001b[36m0:04:00\u001b[0m��━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m30.4/567.8 MB\u001b[0m \u001b[31m2.3 MB/s\u001b[0m eta \u001b[36m0:03:57\u001b[0m��━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m31.2/567.8 MB\u001b[0m \u001b[31m2.3 MB/s\u001b[0m eta \u001b[36m0:03:55\u001b[0m��━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m31.7/567.8 MB\u001b[0m \u001b[31m2.3 MB/s\u001b[0m eta \u001b[36m0:03:54\u001b[0m��━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m32.2/567.8 MB\u001b[0m \u001b[31m2.3 MB/s\u001b[0m eta \u001b[36m0:03:52\u001b[0m��━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m32.5/567.8 MB\u001b[0m \u001b[31m2.3 MB/s\u001b[0m eta \u001b[36m0:03:54\u001b[0m��━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m32.8/567.8 MB\u001b[0m \u001b[31m2.3 MB/s\u001b[0m eta \u001b[36m0:03:55\u001b[0m��━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m33.0/567.8 MB\u001b[0m \u001b[31m2.3 MB/s\u001b[0m eta \u001b[36m0:03:57\u001b[0m��━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m34.1/567.8 MB\u001b[0m \u001b[31m2.3 MB/s\u001b[0m eta \u001b[36m0:03:53\u001b[0m��━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m34.6/567.8 MB\u001b[0m \u001b[31m2.3 MB/s\u001b[0m eta \u001b[36m0:03:52\u001b[0m��━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m35.1/567.8 MB\u001b[0m \u001b[31m2.3 MB/s\u001b[0m eta \u001b[36m0:03:52\u001b[0m��━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m35.7/567.8 MB\u001b[0m \u001b[31m2.3 MB/s\u001b[0m eta \u001b[36m0:03:51\u001b[0m��━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m36.4/567.8 MB\u001b[0m \u001b[31m2.3 MB/s\u001b[0m eta \u001b[36m0:03:48\u001b[0m\n",
                        "\u001b[?25h"
                    ]
                },
                {
                    "ename": "TypeError",
                    "evalue": "%d format: a real number is required, not NoneType",
                    "output_type": "error",
                    "traceback": [
                        "\u001b[31m---------------------------------------------------------------------------\u001b[39m\n",
                        "\u001b[31mCalledProcessError\u001b[39m                        Traceback (most recent call last)\n",
                        "    \u001b[31m[... skipping hidden 1 frame]\u001b[39m\n",
                        "\n",
                        "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[27]\u001b[39m\u001b[32m, line 1\u001b[39m\n",
                        "\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[43mget_ipython\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrun_cell_magic\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mbash\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mpython -m spacy download de_core_news_lg\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[33;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n",
                        "\n",
                        "\u001b[36mFile \u001b[39m\u001b[32m/opt/homebrew/Caskroom/miniconda/base/envs/djpyworkshop/lib/python3.12/site-packages/IPython/core/interactiveshell.py:2539\u001b[39m, in \u001b[36mInteractiveShell.run_cell_magic\u001b[39m\u001b[34m(self, magic_name, line, cell)\u001b[39m\n",
                        "\u001b[32m   2538\u001b[39m     args = (magic_arg_s, cell)\n",
                        "\u001b[32m-> \u001b[39m\u001b[32m2539\u001b[39m     result = \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
                        "\u001b[32m   2541\u001b[39m \u001b[38;5;66;03m# The code below prevents the output from being displayed\u001b[39;00m\n",
                        "\u001b[32m   2542\u001b[39m \u001b[38;5;66;03m# when using magics with decorator @output_can_be_silenced\u001b[39;00m\n",
                        "\u001b[32m   2543\u001b[39m \u001b[38;5;66;03m# when the last Python token in the expression is a ';'.\u001b[39;00m\n",
                        "\n",
                        "\u001b[36mFile \u001b[39m\u001b[32m/opt/homebrew/Caskroom/miniconda/base/envs/djpyworkshop/lib/python3.12/site-packages/IPython/core/magics/script.py:159\u001b[39m, in \u001b[36mScriptMagics._make_script_magic.<locals>.named_script_magic\u001b[39m\u001b[34m(line, cell)\u001b[39m\n",
                        "\u001b[32m    158\u001b[39m     line = script\n",
                        "\u001b[32m--> \u001b[39m\u001b[32m159\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mshebang\u001b[49m\u001b[43m(\u001b[49m\u001b[43mline\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcell\u001b[49m\u001b[43m)\u001b[49m\n",
                        "\n",
                        "\u001b[36mFile \u001b[39m\u001b[32m/opt/homebrew/Caskroom/miniconda/base/envs/djpyworkshop/lib/python3.12/site-packages/IPython/core/magics/script.py:327\u001b[39m, in \u001b[36mScriptMagics.shebang\u001b[39m\u001b[34m(self, line, cell)\u001b[39m\n",
                        "\u001b[32m    326\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m args.raise_error:\n",
                        "\u001b[32m--> \u001b[39m\u001b[32m327\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m CalledProcessError(p.returncode, cell) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n",
                        "\u001b[32m    328\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n",
                        "\n",
                        "\u001b[31m<class 'str'>\u001b[39m: (<class 'TypeError'>, TypeError('%d format: a real number is required, not NoneType'))\n",
                        "\n",
                        "During handling of the above exception, another exception occurred:\n",
                        "\n",
                        "\u001b[31mTypeError\u001b[39m                                 Traceback (most recent call last)\n",
                        "    \u001b[31m[... skipping hidden 1 frame]\u001b[39m\n",
                        "\n",
                        "\u001b[36mFile \u001b[39m\u001b[32m/opt/homebrew/Caskroom/miniconda/base/envs/djpyworkshop/lib/python3.12/site-packages/IPython/core/interactiveshell.py:2181\u001b[39m, in \u001b[36mInteractiveShell.showtraceback\u001b[39m\u001b[34m(self, exc_tuple, filename, tb_offset, exception_only, running_compiled_code)\u001b[39m\n",
                        "\u001b[32m   2178\u001b[39m         traceback.print_exc()\n",
                        "\u001b[32m   2179\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
                        "\u001b[32m-> \u001b[39m\u001b[32m2181\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_showtraceback\u001b[49m\u001b[43m(\u001b[49m\u001b[43metype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstb\u001b[49m\u001b[43m)\u001b[49m\n",
                        "\u001b[32m   2182\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.call_pdb:\n",
                        "\u001b[32m   2183\u001b[39m     \u001b[38;5;66;03m# drop into debugger\u001b[39;00m\n",
                        "\u001b[32m   2184\u001b[39m     \u001b[38;5;28mself\u001b[39m.debugger(force=\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
                        "\n",
                        "\u001b[36mFile \u001b[39m\u001b[32m/opt/homebrew/Caskroom/miniconda/base/envs/djpyworkshop/lib/python3.12/site-packages/ipykernel/zmqshell.py:559\u001b[39m, in \u001b[36mZMQInteractiveShell._showtraceback\u001b[39m\u001b[34m(self, etype, evalue, stb)\u001b[39m\n",
                        "\u001b[32m    553\u001b[39m sys.stdout.flush()\n",
                        "\u001b[32m    554\u001b[39m sys.stderr.flush()\n",
                        "\u001b[32m    556\u001b[39m exc_content = {\n",
                        "\u001b[32m    557\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mtraceback\u001b[39m\u001b[33m\"\u001b[39m: stb,\n",
                        "\u001b[32m    558\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mename\u001b[39m\u001b[33m\"\u001b[39m: \u001b[38;5;28mstr\u001b[39m(etype.\u001b[34m__name__\u001b[39m),\n",
                        "\u001b[32m--> \u001b[39m\u001b[32m559\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mevalue\u001b[39m\u001b[33m\"\u001b[39m: \u001b[38;5;28;43mstr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mevalue\u001b[49m\u001b[43m)\u001b[49m,\n",
                        "\u001b[32m    560\u001b[39m }\n",
                        "\u001b[32m    562\u001b[39m dh = \u001b[38;5;28mself\u001b[39m.displayhook\n",
                        "\u001b[32m    563\u001b[39m \u001b[38;5;66;03m# Send exception info over pub socket for other clients than the caller\u001b[39;00m\n",
                        "\u001b[32m    564\u001b[39m \u001b[38;5;66;03m# to pick up\u001b[39;00m\n",
                        "\n",
                        "\u001b[36mFile \u001b[39m\u001b[32m/opt/homebrew/Caskroom/miniconda/base/envs/djpyworkshop/lib/python3.12/subprocess.py:148\u001b[39m, in \u001b[36mCalledProcessError.__str__\u001b[39m\u001b[34m(self)\u001b[39m\n",
                        "\u001b[32m    145\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33mCommand \u001b[39m\u001b[33m'\u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[33m'\u001b[39m\u001b[33m died with unknown signal \u001b[39m\u001b[38;5;132;01m%d\u001b[39;00m\u001b[33m.\u001b[39m\u001b[33m\"\u001b[39m % (\n",
                        "\u001b[32m    146\u001b[39m                 \u001b[38;5;28mself\u001b[39m.cmd, -\u001b[38;5;28mself\u001b[39m.returncode)\n",
                        "\u001b[32m    147\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n",
                        "\u001b[32m--> \u001b[39m\u001b[32m148\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[33;43m\"\u001b[39;49m\u001b[33;43mCommand \u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[38;5;132;43;01m%s\u001b[39;49;00m\u001b[33;43m'\u001b[39;49m\u001b[33;43m returned non-zero exit status \u001b[39;49m\u001b[38;5;132;43;01m%d\u001b[39;49;00m\u001b[33;43m.\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m \u001b[49m\u001b[43m%\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\n",
                        "\u001b[32m    149\u001b[39m \u001b[43m            \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mcmd\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mreturncode\u001b[49m\u001b[43m)\u001b[49m\n",
                        "\n",
                        "\u001b[31mTypeError\u001b[39m: %d format: a real number is required, not NoneType"
                    ]
                },
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "\n",
                        "\u001b[31mAborted.\u001b[0m\n"
                    ]
                }
            ],
            "source": [
                "# %%bash\n",
                "# python -m spacy download de_core_news_lg"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 1. Data Loading and Basic Statistics"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 1,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Loading spaCy model...\n"
                    ]
                },
                {
                    "ename": "NameError",
                    "evalue": "name 'spacy' is not defined",
                    "output_type": "error",
                    "traceback": [
                        "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
                        "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
                        "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 3\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# Load spaCy model with sentencizer\u001b[39;00m\n\u001b[32m      2\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mLoading spaCy model...\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m----> \u001b[39m\u001b[32m3\u001b[39m nlp = \u001b[43mspacy\u001b[49m.load(\u001b[33m\"\u001b[39m\u001b[33mde_core_news_sm\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m      4\u001b[39m nlp.add_pipe(\u001b[33m'\u001b[39m\u001b[33msentencizer\u001b[39m\u001b[33m'\u001b[39m)  \u001b[38;5;66;03m# Add sentencizer for sentence boundary detection\u001b[39;00m\n\u001b[32m      6\u001b[39m \u001b[38;5;66;03m# Load the text file\u001b[39;00m\n",
                        "\u001b[31mNameError\u001b[39m: name 'spacy' is not defined"
                    ]
                }
            ],
            "source": [
                "# Load spaCy model with sentencizer\n",
                "print(\"Loading spaCy model...\")\n",
                "nlp = spacy.load(\"de_core_news_sm\")\n",
                "nlp.add_pipe('sentencizer')  # Add sentencizer for sentence boundary detection\n",
                "\n",
                "# Load the text file\n",
                "file_path = \"../data/02_text-analysis/parlament-241024.txt\"\n",
                "print(f\"Loading text from: {file_path}\")\n",
                "\n",
                "with open(file_path, 'r', encoding='utf-8') as f:\n",
                "    text = f.read()\n",
                "\n",
                "# Basic text statistics\n",
                "print(f\"\\n📊 Document Statistics:\")\n",
                "print(f\"Total characters: {len(text)}\")\n",
                "print(f\"Total words: {len(text.split())}\")\n",
                "print(f\"Total lines: {len(text.splitlines())}\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Process with spaCy\n",
                "print(\"Processing text with spaCy...\")\n",
                "doc = nlp(text)\n",
                "print(f\"Total tokens: {len(doc)}\")\n",
                "print(f\"Total sentences: {len(list(doc.sents))}\")\n",
                "print(f\"Unique words: {len(set([token.text.lower() for token in doc if token.is_alpha]))}\")\n",
                "\n",
                "# Create a sample preview of the text\n",
                "preview_length = 300\n",
                "text_preview = text[:preview_length] + \"...\" if len(text) > preview_length else text\n",
                "print(f\"\\nText preview: {text_preview}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 2. Word Frequency Analysis"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Word frequency analysis (excluding stopwords and punctuation)\n",
                "print(\"\\nAnalyzing word frequencies...\")\n",
                "word_freq = Counter([token.text.lower() for token in doc if token.is_alpha and not token.is_stop])\n",
                "common_words = pd.DataFrame(word_freq.most_common(20), columns=['Word', 'Frequency'])\n",
                "\n",
                "# Visualization of word frequencies\n",
                "plt.figure(figsize=(12, 6))\n",
                "sns.barplot(x='Frequency', y='Word', data=common_words)\n",
                "plt.title('Most Common Words', fontsize=16)\n",
                "plt.xlabel('Frequency', fontsize=14)\n",
                "plt.ylabel('Word', fontsize=14)\n",
                "plt.tight_layout()\n",
                "plt.show()\n",
                "\n",
                "# Word cloud visualization\n",
                "print(\"\\nGenerating word cloud...\")\n",
                "wordcloud = WordCloud(width=800, height=400,\n",
                "                      background_color='white',\n",
                "                      colormap='viridis',\n",
                "                      collocations=False,\n",
                "                      max_words=200).generate_from_frequencies(word_freq)\n",
                "\n",
                "plt.figure(figsize=(12, 8))\n",
                "plt.imshow(wordcloud, interpolation='bilinear')\n",
                "plt.axis('off')\n",
                "plt.tight_layout()\n",
                "plt.show()"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 3. Topic Analysis\n",
                "\n",
                "Using Latent Dirichlet Allocation (LDA) to identify main topics in the text."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Split text into sentences for topic modeling\n",
                "sentences = [sent.text for sent in doc.sents]\n",
                "\n",
                "# Create document-term matrix using TF-IDF\n",
                "print(\"Creating TF-IDF vectors...\")\n",
                "tfidf_vectorizer = TfidfVectorizer(\n",
                "    max_df=0.95, min_df=2,\n",
                "    stop_words=nlp.Defaults.stop_words\n",
                ")\n",
                "tfidf = tfidf_vectorizer.fit_transform(sentences)\n",
                "feature_names = tfidf_vectorizer.get_feature_names_out()\n",
                "\n",
                "# LDA Topic Modeling\n",
                "n_topics = 5  # Can be adjusted based on expected content\n",
                "print(f\"Running LDA with {n_topics} topics...\")\n",
                "lda = LatentDirichletAllocation(\n",
                "    n_components=n_topics,\n",
                "    random_state=42,\n",
                "    learning_method='online'\n",
                ")\n",
                "lda.fit(tfidf)\n",
                "\n",
                "# Display topics and their top words\n",
                "n_top_words = 10\n",
                "print(f\"\\nTop {n_top_words} words per topic:\")\n",
                "for topic_idx, topic in enumerate(lda.components_):\n",
                "    top_words_idx = topic.argsort()[:-n_top_words - 1:-1]\n",
                "    top_words = [feature_names[i] for i in top_words_idx]\n",
                "    print(f\"Topic #{topic_idx + 1}: {', '.join(top_words)}\")\n",
                "\n",
                "# Interactive topic visualization\n",
                "try:\n",
                "    print(\"\\nGenerating interactive topic visualization...\")\n",
                "    pyLDAvis_data = pyLDAvis.sklearn.prepare(lda, tfidf, tfidf_vectorizer)\n",
                "    pyLDAvis.display(pyLDAvis_data)\n",
                "except Exception as e:\n",
                "    print(f\"Could not generate interactive visualization: {e}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 4. Document Structure Analysis"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Identify document sections based on patterns\n",
                "print(\"\\nAnalyzing document structure...\")\n",
                "\n",
                "# Split into paragraphs\n",
                "paragraphs = [p for p in text.split('\\n\\n') if p.strip()]\n",
                "print(f\"Total paragraphs: {len(paragraphs)}\")\n",
                "\n",
                "# Potential section headers (capitalized lines, numbered sections, etc.)\n",
                "section_pattern = re.compile(r'^([0-9]+\\.\\s+|[A-Z\\s]+:).*$', re.MULTILINE)\n",
                "potential_headers = section_pattern.findall(text)\n",
                "\n",
                "print(f\"Potential section headers identified: {len(potential_headers)}\")\n",
                "if potential_headers:\n",
                "    print(\"Sample headers:\")\n",
                "    for header in potential_headers[:5]:\n",
                "        print(f\"  - {header.strip()}\")\n",
                "\n",
                "# Paragraph length distribution\n",
                "para_lengths = [len(p.split()) for p in paragraphs]\n",
                "plt.figure(figsize=(12, 6))\n",
                "sns.histplot(para_lengths, kde=True, bins=30)\n",
                "plt.axvline(np.mean(para_lengths), color='r', linestyle='--',\n",
                "            label=f'Mean: {np.mean(para_lengths):.2f} words')\n",
                "plt.title('Paragraph Length Distribution', fontsize=16)\n",
                "plt.xlabel('Words per Paragraph', fontsize=14)\n",
                "plt.ylabel('Frequency', fontsize=14)\n",
                "plt.legend()\n",
                "plt.tight_layout()\n",
                "plt.show()\n",
                "\n",
                "# Discourse markers analysis\n",
                "discourse_markers = [\n",
                "    'jedoch', 'allerdings', 'trotzdem', 'dennoch', 'deshalb',\n",
                "    'daher', 'somit', 'folglich', 'außerdem', 'zudem',\n",
                "    'einerseits', 'andererseits', 'zunächst', 'schließlich'\n",
                "]\n",
                "\n",
                "marker_counts = {}\n",
                "for marker in discourse_markers:\n",
                "    pattern = re.compile(r'\\b' + marker + r'\\b', re.IGNORECASE)\n",
                "    marker_counts[marker] = len(pattern.findall(text))\n",
                "\n",
                "# Display discourse marker frequencies\n",
                "marker_df = pd.DataFrame(list(marker_counts.items()), columns=['Discourse Marker', 'Count'])\n",
                "marker_df = marker_df.sort_values('Count', ascending=False)\n",
                "marker_df = marker_df[marker_df['Count'] > 0]  # Only show markers that appear\n",
                "\n",
                "if not marker_df.empty:\n",
                "    plt.figure(figsize=(12, 6))\n",
                "    sns.barplot(x='Count', y='Discourse Marker', data=marker_df)\n",
                "    plt.title('Discourse Marker Frequencies', fontsize=16)\n",
                "    plt.xlabel('Count', fontsize=14)\n",
                "    plt.ylabel('Discourse Marker', fontsize=14)\n",
                "    plt.tight_layout()\n",
                "    plt.show()\n",
                "else:\n",
                "    print(\"No common discourse markers found in the text.\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 5. Named Entity Recognition and Analysis"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Named Entity Recognition analysis\n",
                "print(\"\\nAnalyzing named entities...\")\n",
                "# Filter out single words and common false positives\n",
                "filtered_entities = [\n",
                "    ent for ent in doc.ents\n",
                "    if len(ent.text.strip()) > 3\n",
                "]\n",
                "\n",
                "entity_counts = Counter([ent.label_ for ent in filtered_entities])\n",
                "entity_df = pd.DataFrame(entity_counts.items(), columns=['Entity Type', 'Count'])\n",
                "entity_df = entity_df.sort_values('Count', ascending=False)\n",
                "\n",
                "# Add entity type descriptions\n",
                "entity_descriptions = {\n",
                "    'LOC': 'Location',\n",
                "    'ORG': 'Organization',\n",
                "    'PER': 'Person',\n",
                "    'MISC': 'Miscellaneous',\n",
                "    'DATE': 'Date',\n",
                "    'CARDINAL': 'Cardinal Number',\n",
                "    'MONEY': 'Monetary Value',\n",
                "    'GPE': 'Geopolitical Entity',\n",
                "    'ORDINAL': 'Ordinal Number',\n",
                "    'QUANTITY': 'Quantity'\n",
                "}\n",
                "\n",
                "# Create a new column with descriptions\n",
                "entity_df['Description'] = entity_df['Entity Type'].map(lambda x: entity_descriptions.get(x, x))\n",
                "\n",
                "# Visualization of named entities\n",
                "plt.figure(figsize=(12, 6))\n",
                "sns.barplot(x='Count', y='Entity Type', data=entity_df)\n",
                "plt.title('Distribution of Named Entities', fontsize=16)\n",
                "plt.xlabel('Count', fontsize=14)\n",
                "plt.ylabel('Entity Type', fontsize=14)\n",
                "plt.tight_layout()\n",
                "plt.show()\n",
                "\n",
                "# Display sample named entities\n",
                "print(\"\\n🏷️ Sample Named Entities:\")\n",
                "for i, ent in enumerate(sorted(filtered_entities, key=lambda x: len(x.text), reverse=True)[:15]):\n",
                "    desc = entity_descriptions.get(ent.label_, ent.label_)\n",
                "    print(f\"{ent.text} - {ent.label_} ({desc})\")\n",
                "\n",
                "# Create entity dictionaries by type\n",
                "entities_by_type = {}\n",
                "for ent in filtered_entities:\n",
                "    if ent.label_ not in entities_by_type:\n",
                "        entities_by_type[ent.label_] = []\n",
                "    if ent.text not in entities_by_type[ent.label_]:\n",
                "        entities_by_type[ent.label_].append(ent.text)\n",
                "\n",
                "# Display top entities by type\n",
                "print(\"\\n📋 Top entities by type:\")\n",
                "for ent_type, ents in entities_by_type.items():\n",
                "    if ent_type in entity_descriptions:\n",
                "        type_desc = entity_descriptions[ent_type]\n",
                "        print(f\"\\n{ent_type} ({type_desc}):\")\n",
                "        for ent in sorted(set(ents), key=ents.count, reverse=True)[:5]:\n",
                "            print(f\"  - {ent}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 6. Entity Network Analysis\n",
                "\n",
                "Analyzing relationships between named entities in the text."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "print(\"\\nBuilding entity co-occurrence network...\")\n",
                "# Create a graph of entity co-occurrences within sentences\n",
                "G = nx.Graph()\n",
                "\n",
                "# Track entities by sentence\n",
                "entities_by_sentence = []\n",
                "for sent in doc.sents:\n",
                "    sent_ents = []\n",
                "    sent_doc = nlp(sent.text)  # Re-process to ensure entity recognition\n",
                "    for ent in sent_doc.ents:\n",
                "        if len(ent.text.strip()) > 3:  # Filter short entities\n",
                "            # Add node to graph\n",
                "            if not G.has_node(ent.text):\n",
                "                G.add_node(ent.text, type=ent.label_,\n",
                "                          description=entity_descriptions.get(ent.label_, ent.label_))\n",
                "            sent_ents.append(ent.text)\n",
                "\n",
                "    # Add edges between co-occurring entities\n",
                "    if len(sent_ents) > 1:\n",
                "        entities_by_sentence.append(sent_ents)\n",
                "        for i, ent1 in enumerate(sent_ents):\n",
                "            for ent2 in sent_ents[i+1:]:\n",
                "                if G.has_edge(ent1, ent2):\n",
                "                    G[ent1][ent2]['weight'] += 1\n",
                "                else:\n",
                "                    G.add_edge(ent1, ent2, weight=1)\n",
                "\n",
                "print(f\"Created network with {G.number_of_nodes()} nodes and {G.number_of_edges()} edges\")\n",
                "\n",
                "# Visualize the entity network (if not too large)\n",
                "if 5 <= G.number_of_nodes() <= 100:\n",
                "    plt.figure(figsize=(14, 10))\n",
                "\n",
                "    # Calculate node sizes based on degree centrality\n",
                "    centrality = nx.degree_centrality(G)\n",
                "    node_size = [centrality[node] * 3000 + 100 for node in G.nodes()]\n",
                "\n",
                "    # Calculate edge widths based on weight\n",
                "    edge_width = [G[u][v]['weight'] * 0.5 for u, v in G.edges()]\n",
                "\n",
                "    # Node colors by entity type\n",
                "    color_map = {'PER': 'crimson', 'ORG': 'skyblue', 'LOC': 'green',\n",
                "                'GPE': 'orange', 'DATE': 'purple', 'MISC': 'gray'}\n",
                "    node_colors = [color_map.get(G.nodes[node]['type'], 'gray') for node in G.nodes()]\n",
                "\n",
                "    # Create layout\n",
                "    pos = nx.spring_layout(G, seed=42)\n",
                "\n",
                "    # Draw network components\n",
                "    nx.draw_networkx_nodes(G, pos, node_size=node_size, node_color=node_colors, alpha=0.7)\n",
                "    nx.draw_networkx_edges(G, pos, width=edge_width, alpha=0.5)\n",
                "    nx.draw_networkx_labels(G, pos, font_size=10, font_family=\"sans-serif\")\n",
                "\n",
                "    # Create legend\n",
                "    legend_elements = [plt.Line2D([0], [0], marker='o', color='w',\n",
                "                                 markerfacecolor=color, markersize=10, label=entity_type)\n",
                "                      for entity_type, color in color_map.items()]\n",
                "    plt.legend(handles=legend_elements, title=\"Entity Types\")\n",
                "\n",
                "    plt.title(\"Entity Co-occurrence Network\", fontsize=16)\n",
                "    plt.axis('off')\n",
                "    plt.tight_layout()\n",
                "    plt.show()\n",
                "\n",
                "    # Calculate and display centrality measures\n",
                "    print(\"\\n🔍 Entity Centrality Measures:\")\n",
                "    centrality_df = pd.DataFrame({\n",
                "        'Entity': list(centrality.keys()),\n",
                "        'Degree Centrality': list(centrality.values()),\n",
                "        'Betweenness Centrality': list(nx.betweenness_centrality(G).values()),\n",
                "        'Type': [G.nodes[node]['type'] for node in centrality.keys()]\n",
                "    })\n",
                "    centrality_df = centrality_df.sort_values('Degree Centrality', ascending=False).head(10)\n",
                "    print(centrality_df)\n",
                "else:\n",
                "    if G.number_of_nodes() < 5:\n",
                "        print(\"Too few entities to create a meaningful network.\")\n",
                "    else:\n",
                "        print(\"Network is too large to visualize effectively in this notebook.\")\n",
                "        # Still calculate and display top entities by centrality\n",
                "        centrality = nx.degree_centrality(G)\n",
                "        top_entities = sorted(centrality.items(), key=lambda x: x[1], reverse=True)[:10]\n",
                "        print(\"\\n🔍 Top 10 entities by degree centrality:\")\n",
                "        for entity, score in top_entities:\n",
                "            print(f\"  - {entity}: {score:.4f}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 7. Summary and Key Findings"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "print(\"\\n📝 Summary of Analysis:\")\n",
                "print(f\"- Document contains {len(doc)} tokens in {len(list(doc.sents))} sentences\")\n",
                "print(f\"- Found {len(filtered_entities)} named entities across {len(entity_counts)} different types\")\n",
                "print(f\"- Most common entity type: {entity_df.iloc[0]['Entity Type']} ({entity_df.iloc[0]['Count']} instances)\")\n",
                "print(f\"- Identified {n_topics} main topics in the document\")\n",
                "if G.number_of_nodes() > 0:\n",
                "    print(f\"- Entity network has {G.number_of_nodes()} nodes and {G.number_of_edges()} connections\")\n",
                "print(\"\\nAnalysis complete!\")"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "djpyworkshop",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.12.9"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 4
}
