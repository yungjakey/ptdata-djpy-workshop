{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# Parliamentary Text Analysis\n",
                "\n",
                "This notebook analyzes parliamentary text data using advanced NLP techniques including topic modeling, entity recognition, and network analysis."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Import required libraries\n",
                "import pandas as pd\n",
                "import numpy as np\n",
                "import matplotlib.pyplot as plt\n",
                "import seaborn as sns\n",
                "from collections import Counter\n",
                "import re\n",
                "import spacy\n",
                "from wordcloud import WordCloud\n",
                "import string\n",
                "import networkx as nx\n",
                "from sklearn.feature_extraction.text import TfidfVectorizer\n",
                "from sklearn.decomposition import LatentDirichletAllocation\n",
                "import pyLDAvis\n",
                "import pyLDAvis.sklearn\n",
                "\n",
                "# Set visualization styles\n",
                "plt.style.use('ggplot')\n",
                "sns.set_palette(\"viridis\")\n",
                "sns.set_context(\"notebook\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 1. Data Loading and Basic Statistics"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Load spaCy model with sentencizer\n",
                "print(\"Loading spaCy model...\")\n",
                "nlp = spacy.load(\"de_core_news_sm\")\n",
                "nlp.add_pipe('sentencizer')  # Add sentencizer for sentence boundary detection\n",
                "\n",
                "# Load the text file\n",
                "file_path = \"../data/02_text-analysis/parlament-241024.txt\"\n",
                "print(f\"Loading text from: {file_path}\")\n",
                "\n",
                "with open(file_path, 'r', encoding='utf-8') as f:\n",
                "    text = f.read()\n",
                "\n",
                "# Basic text statistics\n",
                "print(f\"\\nðŸ“Š Document Statistics:\")\n",
                "print(f\"Total characters: {len(text)}\")\n",
                "print(f\"Total words: {len(text.split())}\")\n",
                "print(f\"Total lines: {len(text.splitlines())}\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Process with spaCy\n",
                "print(\"Processing text with spaCy...\")\n",
                "doc = nlp(text)\n",
                "print(f\"Total tokens: {len(doc)}\")\n",
                "print(f\"Total sentences: {len(list(doc.sents))}\")\n",
                "print(f\"Unique words: {len(set([token.text.lower() for token in doc if token.is_alpha]))}\")\n",
                "\n",
                "# Create a sample preview of the text\n",
                "preview_length = 300\n",
                "text_preview = text[:preview_length] + \"...\" if len(text) > preview_length else text\n",
                "print(f\"\\nText preview: {text_preview}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 2. Word Frequency Analysis"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Word frequency analysis (excluding stopwords and punctuation)\n",
                "print(\"\\nAnalyzing word frequencies...\")\n",
                "word_freq = Counter([token.text.lower() for token in doc if token.is_alpha and not token.is_stop])\n",
                "common_words = pd.DataFrame(word_freq.most_common(20), columns=['Word', 'Frequency'])\n",
                "\n",
                "# Visualization of word frequencies\n",
                "plt.figure(figsize=(12, 6))\n",
                "sns.barplot(x='Frequency', y='Word', data=common_words)\n",
                "plt.title('Most Common Words', fontsize=16)\n",
                "plt.xlabel('Frequency', fontsize=14)\n",
                "plt.ylabel('Word', fontsize=14)\n",
                "plt.tight_layout()\n",
                "plt.show()\n",
                "\n",
                "# Word cloud visualization\n",
                "print(\"\\nGenerating word cloud...\")\n",
                "wordcloud = WordCloud(width=800, height=400, \n",
                "                      background_color='white',\n",
                "                      colormap='viridis', \n",
                "                      collocations=False,\n",
                "                      max_words=200).generate_from_frequencies(word_freq)\n",
                "\n",
                "plt.figure(figsize=(12, 8))\n",
                "plt.imshow(wordcloud, interpolation='bilinear')\n",
                "plt.axis('off')\n",
                "plt.tight_layout()\n",
                "plt.show()"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 3. Topic Analysis\n",
                "\n",
                "Using Latent Dirichlet Allocation (LDA) to identify main topics in the text."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Split text into sentences for topic modeling\n",
                "sentences = [sent.text for sent in doc.sents]\n",
                "\n",
                "# Create document-term matrix using TF-IDF\n",
                "print(\"Creating TF-IDF vectors...\")\n",
                "tfidf_vectorizer = TfidfVectorizer(\n",
                "    max_df=0.95, min_df=2,\n",
                "    stop_words=nlp.Defaults.stop_words\n",
                ")\n",
                "tfidf = tfidf_vectorizer.fit_transform(sentences)\n",
                "feature_names = tfidf_vectorizer.get_feature_names_out()\n",
                "\n",
                "# LDA Topic Modeling\n",
                "n_topics = 5  # Can be adjusted based on expected content\n",
                "print(f\"Running LDA with {n_topics} topics...\")\n",
                "lda = LatentDirichletAllocation(\n",
                "    n_components=n_topics, \n",
                "    random_state=42,\n",
                "    learning_method='online'\n",
                ")\n",
                "lda.fit(tfidf)\n",
                "\n",
                "# Display topics and their top words\n",
                "n_top_words = 10\n",
                "print(f\"\\nTop {n_top_words} words per topic:\")\n",
                "for topic_idx, topic in enumerate(lda.components_):\n",
                "    top_words_idx = topic.argsort()[:-n_top_words - 1:-1]\n",
                "    top_words = [feature_names[i] for i in top_words_idx]\n",
                "    print(f\"Topic #{topic_idx + 1}: {', '.join(top_words)}\")\n",
                "\n",
                "# Interactive topic visualization\n",
                "try:\n",
                "    print(\"\\nGenerating interactive topic visualization...\")\n",
                "    pyLDAvis_data = pyLDAvis.sklearn.prepare(lda, tfidf, tfidf_vectorizer)\n",
                "    pyLDAvis.display(pyLDAvis_data)\n",
                "except Exception as e:\n",
                "    print(f\"Could not generate interactive visualization: {e}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 4. Document Structure Analysis"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Identify document sections based on patterns\n",
                "print(\"\\nAnalyzing document structure...\")\n",
                "\n",
                "# Split into paragraphs\n",
                "paragraphs = [p for p in text.split('\\n\\n') if p.strip()]\n",
                "print(f\"Total paragraphs: {len(paragraphs)}\")\n",
                "\n",
                "# Potential section headers (capitalized lines, numbered sections, etc.)\n",
                "section_pattern = re.compile(r'^([0-9]+\\.\\s+|[A-Z\\s]+:).*$', re.MULTILINE)\n",
                "potential_headers = section_pattern.findall(text)\n",
                "\n",
                "print(f\"Potential section headers identified: {len(potential_headers)}\")\n",
                "if potential_headers:\n",
                "    print(\"Sample headers:\")\n",
                "    for header in potential_headers[:5]:\n",
                "        print(f\"  - {header.strip()}\")\n",
                "\n",
                "# Paragraph length distribution\n",
                "para_lengths = [len(p.split()) for p in paragraphs]\n",
                "plt.figure(figsize=(12, 6))\n",
                "sns.histplot(para_lengths, kde=True, bins=30)\n",
                "plt.axvline(np.mean(para_lengths), color='r', linestyle='--', \n",
                "            label=f'Mean: {np.mean(para_lengths):.2f} words')\n",
                "plt.title('Paragraph Length Distribution', fontsize=16)\n",
                "plt.xlabel('Words per Paragraph', fontsize=14)\n",
                "plt.ylabel('Frequency', fontsize=14)\n",
                "plt.legend()\n",
                "plt.tight_layout()\n",
                "plt.show()\n",
                "\n",
                "# Discourse markers analysis\n",
                "discourse_markers = [\n",
                "    'jedoch', 'allerdings', 'trotzdem', 'dennoch', 'deshalb',\n",
                "    'daher', 'somit', 'folglich', 'auÃŸerdem', 'zudem',\n",
                "    'einerseits', 'andererseits', 'zunÃ¤chst', 'schlieÃŸlich'\n",
                "]\n",
                "\n",
                "marker_counts = {}\n",
                "for marker in discourse_markers:\n",
                "    pattern = re.compile(r'\\b' + marker + r'\\b', re.IGNORECASE)\n",
                "    marker_counts[marker] = len(pattern.findall(text))\n",
                "\n",
                "# Display discourse marker frequencies\n",
                "marker_df = pd.DataFrame(list(marker_counts.items()), columns=['Discourse Marker', 'Count'])\n",
                "marker_df = marker_df.sort_values('Count', ascending=False)\n",
                "marker_df = marker_df[marker_df['Count'] > 0]  # Only show markers that appear\n",
                "\n",
                "if not marker_df.empty:\n",
                "    plt.figure(figsize=(12, 6))\n",
                "    sns.barplot(x='Count', y='Discourse Marker', data=marker_df)\n",
                "    plt.title('Discourse Marker Frequencies', fontsize=16)\n",
                "    plt.xlabel('Count', fontsize=14)\n",
                "    plt.ylabel('Discourse Marker', fontsize=14)\n",
                "    plt.tight_layout()\n",
                "    plt.show()\n",
                "else:\n",
                "    print(\"No common discourse markers found in the text.\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 5. Named Entity Recognition and Analysis"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Named Entity Recognition analysis\n",
                "print(\"\\nAnalyzing named entities...\")\n",
                "# Filter out single words and common false positives\n",
                "filtered_entities = [\n",
                "    ent for ent in doc.ents \n",
                "    if len(ent.text.strip()) > 3\n",
                "]\n",
                "\n",
                "entity_counts = Counter([ent.label_ for ent in filtered_entities])\n",
                "entity_df = pd.DataFrame(entity_counts.items(), columns=['Entity Type', 'Count'])\n",
                "entity_df = entity_df.sort_values('Count', ascending=False)\n",
                "\n",
                "# Add entity type descriptions\n",
                "entity_descriptions = {\n",
                "    'LOC': 'Location', \n",
                "    'ORG': 'Organization', \n",
                "    'PER': 'Person',\n",
                "    'MISC': 'Miscellaneous',\n",
                "    'DATE': 'Date',\n",
                "    'CARDINAL': 'Cardinal Number',\n",
                "    'MONEY': 'Monetary Value',\n",
                "    'GPE': 'Geopolitical Entity',\n",
                "    'ORDINAL': 'Ordinal Number',\n",
                "    'QUANTITY': 'Quantity'\n",
                "}\n",
                "\n",
                "# Create a new column with descriptions\n",
                "entity_df['Description'] = entity_df['Entity Type'].map(lambda x: entity_descriptions.get(x, x))\n",
                "\n",
                "# Visualization of named entities\n",
                "plt.figure(figsize=(12, 6))\n",
                "sns.barplot(x='Count', y='Entity Type', data=entity_df)\n",
                "plt.title('Distribution of Named Entities', fontsize=16)\n",
                "plt.xlabel('Count', fontsize=14)\n",
                "plt.ylabel('Entity Type', fontsize=14)\n",
                "plt.tight_layout()\n",
                "plt.show()\n",
                "\n",
                "# Display sample named entities\n",
                "print(\"\\nðŸ·ï¸ Sample Named Entities:\")\n",
                "for i, ent in enumerate(sorted(filtered_entities, key=lambda x: len(x.text), reverse=True)[:15]):\n",
                "    desc = entity_descriptions.get(ent.label_, ent.label_)\n",
                "    print(f\"{ent.text} - {ent.label_} ({desc})\")\n",
                "\n",
                "# Create entity dictionaries by type\n",
                "entities_by_type = {}\n",
                "for ent in filtered_entities:\n",
                "    if ent.label_ not in entities_by_type:\n",
                "        entities_by_type[ent.label_] = []\n",
                "    if ent.text not in entities_by_type[ent.label_]:\n",
                "        entities_by_type[ent.label_].append(ent.text)\n",
                "\n",
                "# Display top entities by type\n",
                "print(\"\\nðŸ“‹ Top entities by type:\")\n",
                "for ent_type, ents in entities_by_type.items():\n",
                "    if ent_type in entity_descriptions:\n",
                "        type_desc = entity_descriptions[ent_type]\n",
                "        print(f\"\\n{ent_type} ({type_desc}):\")\n",
                "        for ent in sorted(set(ents), key=ents.count, reverse=True)[:5]:\n",
                "            print(f\"  - {ent}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 6. Entity Network Analysis\n",
                "\n",
                "Analyzing relationships between named entities in the text."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "print(\"\\nBuilding entity co-occurrence network...\")\n",
                "# Create a graph of entity co-occurrences within sentences\n",
                "G = nx.Graph()\n",
                "\n",
                "# Track entities by sentence\n",
                "entities_by_sentence = []\n",
                "for sent in doc.sents:\n",
                "    sent_ents = []\n",
                "    sent_doc = nlp(sent.text)  # Re-process to ensure entity recognition\n",
                "    for ent in sent_doc.ents:\n",
                "        if len(ent.text.strip()) > 3:  # Filter short entities\n",
                "            # Add node to graph\n",
                "            if not G.has_node(ent.text):\n",
                "                G.add_node(ent.text, type=ent.label_, \n",
                "                          description=entity_descriptions.get(ent.label_, ent.label_))\n",
                "            sent_ents.append(ent.text)\n",
                "    \n",
                "    # Add edges between co-occurring entities\n",
                "    if len(sent_ents) > 1:\n",
                "        entities_by_sentence.append(sent_ents)\n",
                "        for i, ent1 in enumerate(sent_ents):\n",
                "            for ent2 in sent_ents[i+1:]:\n",
                "                if G.has_edge(ent1, ent2):\n",
                "                    G[ent1][ent2]['weight'] += 1\n",
                "                else:\n",
                "                    G.add_edge(ent1, ent2, weight=1)\n",
                "\n",
                "print(f\"Created network with {G.number_of_nodes()} nodes and {G.number_of_edges()} edges\")\n",
                "\n",
                "# Visualize the entity network (if not too large)\n",
                "if 5 <= G.number_of_nodes() <= 100:\n",
                "    plt.figure(figsize=(14, 10))\n",
                "    \n",
                "    # Calculate node sizes based on degree centrality\n",
                "    centrality = nx.degree_centrality(G)\n",
                "    node_size = [centrality[node] * 3000 + 100 for node in G.nodes()]\n",
                "    \n",
                "    # Calculate edge widths based on weight\n",
                "    edge_width = [G[u][v]['weight'] * 0.5 for u, v in G.edges()]\n",
                "    \n",
                "    # Node colors by entity type\n",
                "    color_map = {'PER': 'crimson', 'ORG': 'skyblue', 'LOC': 'green', \n",
                "                'GPE': 'orange', 'DATE': 'purple', 'MISC': 'gray'}\n",
                "    node_colors = [color_map.get(G.nodes[node]['type'], 'gray') for node in G.nodes()]\n",
                "    \n",
                "    # Create layout\n",
                "    pos = nx.spring_layout(G, seed=42)\n",
                "    \n",
                "    # Draw network components\n",
                "    nx.draw_networkx_nodes(G, pos, node_size=node_size, node_color=node_colors, alpha=0.7)\n",
                "    nx.draw_networkx_edges(G, pos, width=edge_width, alpha=0.5)\n",
                "    nx.draw_networkx_labels(G, pos, font_size=10, font_family=\"sans-serif\")\n",
                "    \n",
                "    # Create legend\n",
                "    legend_elements = [plt.Line2D([0], [0], marker='o', color='w', \n",
                "                                 markerfacecolor=color, markersize=10, label=entity_type)\n",
                "                      for entity_type, color in color_map.items()]\n",
                "    plt.legend(handles=legend_elements, title=\"Entity Types\")\n",
                "    \n",
                "    plt.title(\"Entity Co-occurrence Network\", fontsize=16)\n",
                "    plt.axis('off')\n",
                "    plt.tight_layout()\n",
                "    plt.show()\n",
                "    \n",
                "    # Calculate and display centrality measures\n",
                "    print(\"\\nðŸ” Entity Centrality Measures:\")\n",
                "    centrality_df = pd.DataFrame({\n",
                "        'Entity': list(centrality.keys()),\n",
                "        'Degree Centrality': list(centrality.values()),\n",
                "        'Betweenness Centrality': list(nx.betweenness_centrality(G).values()),\n",
                "        'Type': [G.nodes[node]['type'] for node in centrality.keys()]\n",
                "    })\n",
                "    centrality_df = centrality_df.sort_values('Degree Centrality', ascending=False).head(10)\n",
                "    print(centrality_df)\n",
                "else:\n",
                "    if G.number_of_nodes() < 5:\n",
                "        print(\"Too few entities to create a meaningful network.\")\n",
                "    else:\n",
                "        print(\"Network is too large to visualize effectively in this notebook.\")\n",
                "        # Still calculate and display top entities by centrality\n",
                "        centrality = nx.degree_centrality(G)\n",
                "        top_entities = sorted(centrality.items(), key=lambda x: x[1], reverse=True)[:10]\n",
                "        print(\"\\nðŸ” Top 10 entities by degree centrality:\")\n",
                "        for entity, score in top_entities:\n",
                "            print(f\"  - {entity}: {score:.4f}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 7. Summary and Key Findings"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "print(\"\\nðŸ“ Summary of Analysis:\")\n",
                "print(f\"- Document contains {len(doc)} tokens in {len(list(doc.sents))} sentences\")\n",
                "print(f\"- Found {len(filtered_entities)} named entities across {len(entity_counts)} different types\")\n",
                "print(f\"- Most common entity type: {entity_df.iloc[0]['Entity Type']} ({entity_df.iloc[0]['Count']} instances)\")\n",
                "print(f\"- Identified {n_topics} main topics in the document\")\n",
                "if G.number_of_nodes() > 0:\n",
                "    print(f\"- Entity network has {G.number_of_nodes()} nodes and {G.number_of_edges()} connections\")\n",
                "print(\"\\nAnalysis complete!\")"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.9.7"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 4
}